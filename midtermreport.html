<h3><a href="index.html">Concept</a><a href="Implementation.html">Implementation</a> <a href="Results.html">Results</a> <a href="midtermreport.html">Midterm Report</a></h3>
<body>
  <h1>Midterm Report</h1>

Photo manipulation is for the sake of improving the quality of an image. When processing an image around its edge pixels all photographic filters encounter the issue of not being able to gather information outside of the photo’s boundaries and, without intervention, will result in a smaller image on the output. Filters are prevalent in many popular social media networks like Instagram and snapchat where shrinking is dependent on filter size and will be less noticeable with larger image sensors; however with non-professional, consumer-facing cameras - such as those used for most social media engagements - a smaller photo can result in the removal of significant features in a photo. The shrinking on professional grade cameras may be insignificant but maintaining consistency across image size and aspect ratio when creating professional work is important. Looking forward, with enough training this project could be used to reduce the cropping in digitally stitched panoramic photos, filling in the pixels without values that result from perspective warping and again maintaining the aspect ratio. This sort of prediction based “hallucinating” could also be used to intelligently remove objects like signs, construction cones, or other visually unattractive subjects in a photo.

Filters are appearing in more applications from professional photo manipulation and visual software like Affinity or Photoshop, to marketing platforms like Snapchat, or Social Media platforms like Instagram and Facebook. Because current methods used to fix the issue surrounding edge pixels are presumptuous, the quality and accuracy of the applied filter is reduced. By extending the edges of a photo, the filter is able to use information that is directly informed by the subjects of the photo. With sufficient training, this application could also be used to assist in seam matching in panoramic photos, improving the accuracy of stitching and reducing the amount of photos necessary for matching and creating a quality panorama as well as properly warping perspective between each photo.

Currently, there are 4 methods to apply convolution to border pixels of an image. The most common method takes each border pixels’ information, copies it, and extends it out past the image as far as necessary. This only works well if the image has straight lines, otherwise the convolution will not be accurate. The first image to the right is an example, exaggerated so we can actually see the problem, of what border pixels will be computed with in extension. This method works fine but is not ideal as not all lines in images are horizontal. Another method used on border pixels is a wrap function where pixels are used from the opposite side of the image in convolution. Wrapping the pixels to the other side of the image would work great for a symmetric image, however this is rarely the case. For example, the second image has very different features on the right and left sides. When wrapped, the blue pixels on the right border would be convoluted with the dark grey/black pixels located on the left side of the image. This would cause undesirable effects and is not always the best option for convolution of border pixels. A third method that is commonly used is mirroring the edge pixels back on themselves. This is the best method presented, however it still does not work well with angled objects in the image. If the object goes outside the border and is reflected back upon itself, it does not represent how the object likely continues off the scope of the image. Thus it does not appropriately perform the desired convolution. The last method that is commonly used is to ignore the fact that nothing lies beyond the border of the image. This means the border pixels have half of the amount of pixels required to do a proper convolution. Therefore, the results of the applied filter will be dramatically reduced on the border pixels compared to standard pixels on the interior of the image.

We are going to attempt an entirely new approach to the problem of image extension. None of the current methods that exist can accurately account for lines and other features meeting the border at unknown angles. To try and fix this problem that exists in all existing methods for image extension, we are going to train a neural network on a dataset of images and copies of those images that have been cropped by a set number of pixels, determined by your application or required filter size. We won’t be training the neural network on just the plain images though, instead we are going to train on the Fourier Transforms of the training images. This is because we can keep a standard Fourier Transform matrix size while being able to describe any size input image. This will make it easier to build a neural network since now all of the required inputs and outputs are matrices of the same dimensions. Additionally, the Fourier Transform organizes information about the image by feature frequency, ie edges use higher frequencies. This may potentially help the neural network learn how to extend images especially for applications like image filtering because general edge direction is important in this application and even if the prediction is rough it may be better on average than previous techniques. Once the network is trained it will hopefully accurately predict pixels outside the actual image, but because of the process we used, an additional step is required to use the trained network. The output of the neural network most likely has many of its original pixels changed, so we should take only the edge pixels from the output (after transforming it back to a normal image) and add them to the edges of the input image. This will create an image with accurate original pixels and exterior pixels predicted by our neural network which, among other applications, can now be used to more accurately filter an image.

Existing solutions to filtering edge pixels remain static and assume that the qualities of the edge continue linearly from the edge. Organic shapes such as leaves of trees, hair, or clouds will not necessarily continue past the edge in the manner that mirroring will assume. By determining the form of the subjects of the image, our neural net will predict the way in which these organic forms will continue beyond the boundaries captured in the photo.

To test the accuracy of our method we will simply create a testing set, completely different from the training image set, that will be made of original images paired with cropped copies of themselves. Then we will run this testing set through our neural network and alternative image extension techniques and compare the extended pixels, the pixels created outside the original image by different extension methods, to the true values of those pixels in the testing set. Doing this over a variety of images and crop amounts will show which methods turn out to be the most accurate.
</body>
