
##Motivation

Convolutional filters are a very foundational part of image computation. For those who don’t know what a convolutional filter is, they can be thought of as creating an image pixel-by-pixel by combining a nxn grid of pixels with a nxn intensity filter applied for each pixel. For each pixel of the new image, the grid of considered image pixels moves correspondingly. In other words, to calculate the value at positions (0,0) the center of the considered grid will be at (0,0) on the original image and for position (1,0) the center of the grid will move to (1,0) on the original image.

These two pixel positions are ones of many that highlight an interesting issue with convolutional filters. Consider computing the first position of a convolutionaly filtered image, starting at the top left corner or position (0,0). Let's say we only need a 5x5 grid to compute the filter. At position (0,0) the grid will be over 9 known image pixels and 16 unknown pixels outside the bounds of the image. This issue of missing values affects all pixels around the border of an image. The question is what is the best way to treat those pixels so to compute an accurate convolutional filter. 

Current techniques include: setting all unknown pixels to the same value, mirroring the image over the border, and stretching the edge pixels. These techniques allow for decent convolutional filters to be applied, but they all have different image cases they don’t work as well in. The main issue with all of these techniques is that they do not preserve the direction of edges leaving images. Picture an angled tree branch that gets cut off by the edge of the image. None of the existing image extension techniques will accurately account for the direction of that branch. The mirror technique will flip the branch angle over the horizontal axis and the edge stretch technique will be treat the branch like it suddenly jets off horizontally. Neither of these produce a desired result which will affect the quality and accuracy of the computed filtered image.

Our thought is that a neural network should be able to be trained to accurately account for these edge direction since the information needed to preserve them exists within the image. The hope is that with a good enough training algorithm and large enough training set of images a neural network will begin to pick up on how accurately extend image boundaries.

##Approach and Implementation

The immediate issue with training a neural network to extend the boundaries of an image is that it is impossible to feed different sized matrices, or images in our case, into most if not all conventional graph designs. This would mean that all image sizes would need a separate network trained and on top of this you would also need to train each image size with a variety of output sizes to accommodate different filter sizes. Luckily we have Fourier transforms.

Instead of inputting normal images into the neural network, we can instead perform a fourier transform on the image and input that into the neural network. We can Fourier transform all images into the same dimension matrix so that they can all be input into the same neural network. This does mean that quality of images will be capped at whatever sized Fourier domain is used. This is not that impactful to our use as we are mainly interested in the larger pattern of the edge directions. An additional thought was that since Fourier transforms can highlight edges in images with the use of a high pass filter that it may organize the information in a more useable way for the neural network. 

The training set for the neural network will be made by taking a random selection of images of a certain style, we chose landscape photographs for ours, and center cropping them to create input images. If the network is going to be general use then a range of cropping amounts can be used, but the network will likely be more accurate for set crop amount. We chose to use a crop amount of 24 pixels per edge which would allow for up to a 49x49 pixel convolutional filter to be used with this network. 

The cropped and uncropped image pairs were then filtered to be grayscale, they could also be separated into color channels and trained on separate networks for potential gains in accuracy for colored photos. Now as grayscale images, the cropped and uncropped images are Fourier transformed to 1000x1000 pixel Fourier domains. The Fourier Transform of the cropped image was saved as FInputXXXX.png and the Fourier Transform of the uncropped image was saved as FExpectedXXXX.png with the XXXX’s being the iterative image number and the same for each pair of images.

A sequential neural network was set up in JavaScript using TensorFlow.js to train on the Fourier Transformed image pairs. We then trained the network with ~2000 Fourier Transform image pairs of landscape scenery. Once trained, the network can be used on novel images by first Fourier Transforming them and inputting them into the neural network. The NN outputs a 1000x1000 Fourier Domain of the predicted larger image. In our networks case it will be a prediction for the input image extended by 24 pixels on all sides. This Fourier Transformed prediction is then transformed back to a normal image. Since all this manipulation likely messed up a lot of the original image’s pixels, we just trim off the new border pixels and fill them in around the original image to get a rough makeup of the nonexistent pixels. 

This image with grown pixels can now be used with a convolutional filter to hopefully get a more accurate filtered image than previous techniques allowed.

To somewhat evaluate the results of our neural network we will compare an average pixel prediction score for each technique across a variety of random images. 

##Results
After all was said and done our neural network did not perform better than any existing technique. We attribute this mostly to lack of knowledge about TensorFlow and lack of time to learn all of the ins and outs of it. 

The predicted pixels for our images appeared to be randomly made and did not account for boundary edge direction as we had hoped. The randomness of the result makes us believe that we were not using a viable NN configuration or training algorithm, or we may not have trained the network on enough images. We still believe that this technique will work in theory, but we were unable to make it work ourselves.

##Problems Encountered

The main issues we came across were in the implementation of the neural network. We decided early on to use TensorFlow and struggled probably too much to get it to work. In hindsight we probably should have explored other options for our neural network when we would have still had time to switch over. We don’t believe that there is anything inherently wrong with TensorFlow that prevents it from solving this problem, but our knowledge is limited on how to use its capabilities.

We ran into a few other issues along the way. It took us a while to realize that we should be using grayscale images, so that set us back when we had to recreate the datasets. The Fourier Transform of a colored image has three dimensions, so we had a lot of confusion for a while as to why formats were messed up and why the matrix dimensions were off. We solved these issues by using grayscale images for the Fourier Transformations.

#Concept
When applying filters to an image using convolution it is not currently possible to apply the filter at the boundary of the image and they must be dealt with in a different way. This is because when applying a convolutional filter, each pixel is determined from a function of the surrounding pixels and the filter. If any of the surrounding pixels are missing, as is the case for boundary pixels, the convolution cannot be properly calculated. We covered some possible solutions to this problem in lecture, including simply ignoring these pixels or mirroring the image across the image border to fill the pixels there, but it was noted that all of these solutions are imperfect. When ignoring boundary pixels, the pixels around the border are discarded and the resulting image shrinks. Mirroring the image across the border allows you to calculate the border pixels and avoid shrinking the image, but the accuracy of the filter depends on how close the mirrored pixels are to the ones that would actually be there had the image originally been larger. 
The goal of our project is to develop a program that will add pixels around the border of an image, in other words it will grow the image, thus allowing a filter to be more accurately applied while not losing the photo’s original field of view. We believe that we can train a neural network to predict pixels just outside of an image, so that the pixels on the border of an image can be properly convoluted like any interior pixel. The accuracy of the applied filter will be dependent on the accuracy of the pixels predicted by the neural network.
    This is an important issue to solve because pixels outside an image are needed to apply any convolutional filter to any image, and we have seen that existing solutions have their flaws. We think our program could be applied to allow for higher quality editing and, depending on the accuracy, it could also be directly used to allow a computer to predict, or see, beyond the edges of the image which could have further reaching computer vision applications. 
We plan on training a neural network to achieve this result with a training set of our own images, varying in style and content. Each image will be preprocessed to trim of n-pixels of the image border to create a set of input images tied to expected output images. The n-pixels required to trim correspond to floor(filterWidth/2) when training to apply a filter assuming its square, or simply how many pixels you want to extend the image by. Some rescaling of input and output images may also be required to properly work with our neural network. The hope is that with a big enough data set, the network should start to make correct predictions of pixels outside any image. For the scope of this assignment however, we may decide to narrow to a specific type of photograph such that we can have a more specific training set and make more interesting pixel predictions for that type of photograph.

#Midterm Report

 Photo manipulation is for the sake of improving the quality of an image. When processing an image around its edge pixels all photographic filters encounter the issue of not being able to gather information outside of the photo’s boundaries and, without intervention, will result in a smaller image on the output. Filters are prevalent in many popular social media networks like Instagram and snapchat where shrinking is dependent on filter size and will be less noticeable with larger image sensors; however with non-professional, consumer-facing cameras - such as those used for most social media engagements - a smaller photo can result in the removal of significant features in a photo. The shrinking on professional grade cameras may be insignificant but maintaining consistency across image size and aspect ratio when creating professional work is important.
Looking forward, with enough training this project could be used to reduce the cropping in digitally stitched panoramic photos, filling in the pixels without values that result from perspective warping and again maintaining the aspect ratio. This sort of prediction based “hallucinating” could also be used to intelligently remove objects like signs, construction cones, or other visually unattractive subjects in a photo.

 Filters are appearing in more applications from professional photo manipulation and visual software like Affinity or Photoshop, to marketing platforms like Snapchat, or Social Media platforms like Instagram and Facebook. Because current methods used to fix the issue surrounding edge pixels are presumptuous, the quality and accuracy of the applied filter is reduced. By extending the edges of a photo, the filter is able to use information that is directly informed by the subjects of the photo. 
With sufficient training, this application could also be used to assist in seam matching in panoramic photos, improving the accuracy of stitching and reducing the amount of photos necessary for matching and creating a quality panorama as well as properly warping perspective between each photo.

 Currently, there are 4 methods to apply convolution to border pixels of an image. The most common method takes each border pixels’ information, copies it, and extends it out past the image as far as necessary. This only works well if the image has straight lines, otherwise the convolution will not be accurate. The first image to the right is an example, exaggerated so we can actually see the problem, of what border pixels will be computed with in extension. This method works fine but is not ideal as not all lines in images are horizontal. Another method used on border pixels is a wrap function where pixels are used from the opposite side of the image in convolution. Wrapping the pixels to the other side of the image would work great for a symmetric image, however this is rarely the case. For example, the second image has very different features on the right and left sides. When wrapped, the blue pixels on the right border would be convoluted with the dark grey/black pixels located on the left side of the image. This would cause undesirable effects and is not always the best option for convolution of border pixels. A third method that is commonly used is mirroring the edge pixels back on themselves. This is the best method presented, however it still does not work well with angled objects in the image. If the object goes outside the border and is reflected back upon itself, it does not represent how the object likely continues off the scope of the image. Thus it does not appropriately perform the desired convolution. The last method that is commonly used is to ignore the fact that nothing lies beyond the border of the image. This means the border pixels have half of the amount of pixels required to do a proper convolution. Therefore, the results of the applied filter will be dramatically reduced on the border pixels compared to standard pixels on the interior of the image.

 We are going to attempt an entirely new approach to the problem of image extension. None of the current methods that exist can accurately account for lines and other features meeting the border at unknown angles. To try and fix this problem that exists in all existing methods for image extension, we are going to train a neural network on a dataset of images and copies of those images that have been cropped by a set number of pixels, determined by your application or required filter size. We won't be training the neural network on just the plain images though, instead we are going to train on the Fourier Transforms of the training images. This is because we can keep a standard Fourier Transform matrix size while being able to describe any size input image. This will make it easier to build a neural network since now all of the required inputs and outputs are matrices of the same dimensions. Additionally, the Fourier Transform organizes information about the image by feature frequency, ie edges use higher frequencies. This may potentially help the neural network learn how to extend images especially for applications like image filtering because general edge direction is important in this application and even if the prediction is rough it may be better on average than previous techniques.
    Once the network is trained it will hopefully accurately predict pixels outside the actual image, but because of the process we used, an additional step is required to use the trained network. The output of the neural network most likely has many of its original pixels changed, so we should take only the edge pixels from the output (after transforming it back to a normal image) and add them to the edges of the input image. This will create an image with accurate original pixels and exterior pixels predicted by our neural network which, among other applications, can now be used to more accurately filter an image. 

 Existing solutions to filtering edge pixels remain static and assume that the qualities of the edge continue linearly from the edge. Organic shapes such as leaves of trees, hair, or clouds will not necessarily continue past the edge in the manner that mirroring will assume. By determining the form of the subjects of the image, our neural net will predict the way in which these organic forms will continue beyond the boundaries captured in the photo.

 To test the accuracy of our method we will simply create a testing set, completely different from the training image set, that will be made of original images paired with cropped copies of themselves. Then we will run this testing set through our neural network and alternative image extension techniques and compare the extended pixels, the pixels created outside the original image by different extension methods, to the true values of those pixels in the testing set. Doing this over a variety of images and crop amounts will show which methods turn out to be the most accurate.
