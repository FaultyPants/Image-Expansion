#Concept
When applying filters to an image using convolution it is not currently possible to apply the filter at the boundary of the image and they must be dealt with in a different way. This is because when applying a convolutional filter, each pixel is determined from a function of the surrounding pixels and the filter. If any of the surrounding pixels are missing, as is the case for boundary pixels, the convolution cannot be properly calculated. We covered some possible solutions to this problem in lecture, including simply ignoring these pixels or mirroring the image across the image border to fill the pixels there, but it was noted that all of these solutions are imperfect. When ignoring boundary pixels, the pixels around the border are discarded and the resulting image shrinks. Mirroring the image across the border allows you to calculate the border pixels and avoid shrinking the image, but the accuracy of the filter depends on how close the mirrored pixels are to the ones that would actually be there had the image originally been larger. 
The goal of our project is to develop a program that will add pixels around the border of an image, in other words it will grow the image, thus allowing a filter to be more accurately applied while not losing the photo’s original field of view. We believe that we can train a neural network to predict pixels just outside of an image, so that the pixels on the border of an image can be properly convoluted like any interior pixel. The accuracy of the applied filter will be dependent on the accuracy of the pixels predicted by the neural network.

    This is an important issue to solve because pixels outside an image are needed to apply any convolutional filter to any image, and we have seen that existing solutions have their flaws. We think our program could be applied to allow for higher quality editing and, depending on the accuracy, it could also be directly used to allow a computer to predict, or see, beyond the edges of the image which could have further reaching computer vision applications. 
We plan on training a neural network to achieve this result with a training set of our own images, varying in style and content. Each image will be preprocessed to trim of n-pixels of the image border to create a set of input images tied to expected output images. The n-pixels required to trim correspond to floor(filterWidth/2) when training to apply a filter assuming its square, or simply how many pixels you want to extend the image by. Some rescaling of input and output images may also be required to properly work with our neural network. The hope is that with a big enough data set, the network should start to make correct predictions of pixels outside any image. For the scope of this assignment however, we may decide to narrow to a specific type of photograph such that we can have a more specific training set and make more interesting pixel predictions for that type of photograph.

#Midterm Report

 Photo manipulation is for the sake of improving the quality of an image. When processing an image around its edge pixels all photographic filters encounter the issue of not being able to gather information outside of the photo’s boundaries and, without intervention, will result in a smaller image on the output. Filters are prevalent in many popular social media networks like Instagram and snapchat where shrinking is dependent on filter size and will be less noticeable with larger image sensors; however with non-professional, consumer-facing cameras - such as those used for most social media engagements - a smaller photo can result in the removal of significant features in a photo. The shrinking on professional grade cameras may be insignificant but maintaining consistency across image size and aspect ratio when creating professional work is important.
Looking forward, with enough training this project could be used to reduce the cropping in digitally stitched panoramic photos, filling in the pixels without values that result from perspective warping and again maintaining the aspect ratio. This sort of prediction based “hallucinating” could also be used to intelligently remove objects like signs, construction cones, or other visually unattractive subjects in a photo.

 Filters are appearing in more applications from professional photo manipulation and visual software like Affinity or Photoshop, to marketing platforms like Snapchat, or Social Media platforms like Instagram and Facebook. Because current methods used to fix the issue surrounding edge pixels are presumptuous, the quality and accuracy of the applied filter is reduced. By extending the edges of a photo, the filter is able to use information that is directly informed by the subjects of the photo. 
With sufficient training, this application could also be used to assist in seam matching in panoramic photos, improving the accuracy of stitching and reducing the amount of photos necessary for matching and creating a quality panorama as well as properly warping perspective between each photo.

 Currently, there are 4 methods to apply convolution to border pixels of an image. The most common method takes each border pixels’ information, copies it, and extends it out past the image as far as necessary. This only works well if the image has straight lines, otherwise the convolution will not be accurate. The first image to the right is an example, exaggerated so we can actually see the problem, of what border pixels will be computed with in extension. This method works fine but is not ideal as not all lines in images are horizontal. Another method used on border pixels is a wrap function where pixels are used from the opposite side of the image in convolution. Wrapping the pixels to the other side of the image would work great for a symmetric image, however this is rarely the case. For example, the second image has very different features on the right and left sides. When wrapped, the blue pixels on the right border would be convoluted with the dark grey/black pixels located on the left side of the image. This would cause undesirable effects and is not always the best option for convolution of border pixels. A third method that is commonly used is mirroring the edge pixels back on themselves. This is the best method presented, however it still does not work well with angled objects in the image. If the object goes outside the border and is reflected back upon itself, it does not represent how the object likely continues off the scope of the image. Thus it does not appropriately perform the desired convolution. The last method that is commonly used is to ignore the fact that nothing lies beyond the border of the image. This means the border pixels have half of the amount of pixels required to do a proper convolution. Therefore, the results of the applied filter will be dramatically reduced on the border pixels compared to standard pixels on the interior of the image.

 We are going to attempt an entirely new approach to the problem of image extension. None of the current methods that exist can accurately account for lines and other features meeting the border at unknown angles. To try and fix this problem that exists in all existing methods for image extension, we are going to train a neural network on a dataset of images and copies of those images that have been cropped by a set number of pixels, determined by your application or required filter size. We won't be training the neural network on just the plain images though, instead we are going to train on the Fourier Transforms of the training images. This is because we can keep a standard Fourier Transform matrix size while being able to describe any size input image. This will make it easier to build a neural network since now all of the required inputs and outputs are matrices of the same dimensions. Additionally, the Fourier Transform organizes information about the image by feature frequency, ie edges use higher frequencies. This may potentially help the neural network learn how to extend images especially for applications like image filtering because general edge direction is important in this application and even if the prediction is rough it may be better on average than previous techniques.
    Once the network is trained it will hopefully accurately predict pixels outside the actual image, but because of the process we used, an additional step is required to use the trained network. The output of the neural network most likely has many of its original pixels changed, so we should take only the edge pixels from the output (after transforming it back to a normal image) and add them to the edges of the input image. This will create an image with accurate original pixels and exterior pixels predicted by our neural network which, among other applications, can now be used to more accurately filter an image. 

 Existing solutions to filtering edge pixels remain static and assume that the qualities of the edge continue linearly from the edge. Organic shapes such as leaves of trees, hair, or clouds will not necessarily continue past the edge in the manner that mirroring will assume. By determining the form of the subjects of the image, our neural net will predict the way in which these organic forms will continue beyond the boundaries captured in the photo.

 To test the accuracy of our method we will simply create a testing set, completely different from the training image set, that will be made of original images paired with cropped copies of themselves. Then we will run this testing set through our neural network and alternative image extension techniques and compare the extended pixels, the pixels created outside the original image by different extension methods, to the true values of those pixels in the testing set. Doing this over a variety of images and crop amounts will show which methods turn out to be the most accurate.
